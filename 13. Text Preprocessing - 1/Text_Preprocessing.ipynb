{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neU5I9ePH0KS",
        "outputId": "b16cffa7-8928-4045-e490-fa40e8112cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries from nltk\n",
        "import nltk                                 # natural language toolkit\n",
        "nltk.download('punkt')                      # requirements\n",
        "nltk.download('wordnet')                    # requirements\n",
        "from nltk.tokenize import sent_tokenize     # corpus ---> document\n",
        "from nltk.tokenize import word_tokenize     # document --> word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WymQK3iVIIdw",
        "outputId": "c04cc3f4-a135-4853-eaae-efbd2822a587"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = '''Contrary to popular belief!, Lorem Ipsum is not simply random text.\n",
        "It has root's in a piece of classical Latin literature from 45 BC,\n",
        "making it over 2000 years old!. Richard's McClintock, a Latin professor at\n",
        "Hampden-Sydney in Virginia, looked up one of the more obscure Latin words.'''\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcf8v09eIpSB",
        "outputId": "b1755a98-b895-4c66-fd2a-535c444cd09d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrary to popular belief!, Lorem Ipsum is not simply random text.\n",
            "It has root's in a piece of classical Latin literature from 45 BC,\n",
            "making it over 2000 years old!. Richard's McClintock, a Latin professor at\n",
            "Hampden-Sydney in Virginia, looked up one of the more obscure Latin words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# senetence tokenization to convert corpus into documents\n",
        "documents = sent_tokenize(corpus)\n",
        "print(documents)\n",
        "print()\n",
        "\n",
        "# looping to see individual document\n",
        "num = 1\n",
        "for sentence in documents:\n",
        "    print(f'document-{num}:',sentence)\n",
        "    num += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XSbWGfKI6do",
        "outputId": "9aa6f7c8-6589-4128-ea47-96ef6c76ca4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contrary to popular belief!, Lorem Ipsum is not simply random text.', \"It has root's in a piece of classical Latin literature from 45 BC,\\nmaking it over 2000 years old!.\", \"Richard's McClintock, a Latin professor at\\nHampden-Sydney in Virginia, looked up one of the more obscure Latin words.\"]\n",
            "\n",
            "document-1: Contrary to popular belief!, Lorem Ipsum is not simply random text.\n",
            "document-2: It has root's in a piece of classical Latin literature from 45 BC,\n",
            "making it over 2000 years old!.\n",
            "document-3: Richard's McClintock, a Latin professor at\n",
            "Hampden-Sydney in Virginia, looked up one of the more obscure Latin words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to convert document into words\n",
        "# traditional method by loops\n",
        "for sentence in documents:\n",
        "    print(word_tokenize(sentence))\n",
        "\n",
        "# special character (hyphen) between the words are not split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdgvsy_xJ4hg",
        "outputId": "9c34bb44-f2c1-4cba-93e0-3eb90ef9287e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contrary', 'to', 'popular', 'belief', '!', ',', 'Lorem', 'Ipsum', 'is', 'not', 'simply', 'random', 'text', '.']\n",
            "['It', 'has', 'root', \"'s\", 'in', 'a', 'piece', 'of', 'classical', 'Latin', 'literature', 'from', '45', 'BC', ',', 'making', 'it', 'over', '2000', 'years', 'old', '!', '.']\n",
            "['Richard', \"'s\", 'McClintock', ',', 'a', 'Latin', 'professor', 'at', 'Hampden-Sydney', 'in', 'Virginia', ',', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'Latin', 'words', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to over come the above issue\n",
        "\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "for sentence in documents:\n",
        "    print(wordpunct_tokenize(sentence))\n",
        "\n",
        "# special character and the dot is not splitted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDyqQlfFKYsb",
        "outputId": "6a6ac86e-e973-41be-a47f-a55b7dca4d21"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contrary', 'to', 'popular', 'belief', '!,', 'Lorem', 'Ipsum', 'is', 'not', 'simply', 'random', 'text', '.']\n",
            "['It', 'has', 'root', \"'\", 's', 'in', 'a', 'piece', 'of', 'classical', 'Latin', 'literature', 'from', '45', 'BC', ',', 'making', 'it', 'over', '2000', 'years', 'old', '!.']\n",
            "['Richard', \"'\", 's', 'McClintock', ',', 'a', 'Latin', 'professor', 'at', 'Hampden', '-', 'Sydney', 'in', 'Virginia', ',', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'Latin', 'words', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to overcome the above issue\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokens = TreebankWordTokenizer()\n",
        "for sentence in documents:\n",
        "    print(tokens.tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_llqrK55K9Ww",
        "outputId": "333247bb-99cb-475c-882b-a382bf78ec34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contrary', 'to', 'popular', 'belief', '!', ',', 'Lorem', 'Ipsum', 'is', 'not', 'simply', 'random', 'text', '.']\n",
            "['It', 'has', 'root', \"'s\", 'in', 'a', 'piece', 'of', 'classical', 'Latin', 'literature', 'from', '45', 'BC', ',', 'making', 'it', 'over', '2000', 'years', 'old', '!', '.']\n",
            "['Richard', \"'s\", 'McClintock', ',', 'a', 'Latin', 'professor', 'at', 'Hampden-Sydney', 'in', 'Virginia', ',', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'Latin', 'words', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING - To find the root word"
      ],
      "metadata": {
        "id": "mt2E7N1KL4ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = ['Programming', 'Programs', 'Acheive', 'Acheiving', 'enjoyment', 'eagerly',\n",
        "            'enjoying', 'enjoyed', 'History', 'Historical','eating', 'eaten', 'orderly']"
      ],
      "metadata": {
        "id": "MxW4dz6bMBQr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmed = PorterStemmer().stem\n",
        "for word in word_list:\n",
        "    print(word, '=', stemmed(word))\n",
        "\n",
        "\n",
        "# not all the root words are identified\n",
        "# we ca use porterstemmer for natural language understanding (siri, alexa, google assistance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wft1IjjIMfzg",
        "outputId": "c89f26f6-0510-48c7-ad9e-fff60adc3e8d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programming = program\n",
            "Programs = program\n",
            "Acheive = acheiv\n",
            "Acheiving = acheiv\n",
            "enjoyment = enjoy\n",
            "eagerly = eagerli\n",
            "enjoying = enjoy\n",
            "enjoyed = enjoy\n",
            "History = histori\n",
            "Historical = histor\n",
            "eating = eat\n",
            "eaten = eaten\n",
            "orderly = orderli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snow_ball = SnowballStemmer('english').stem\n",
        "for word in word_list:\n",
        "    print(word, '=', snow_ball(word))\n",
        "\n",
        "# we can use snowball stemmer when we want to exclude the NLU consideration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eFt-vx9NUkQ",
        "outputId": "e680c8df-be7e-41f3-f74f-e38089608696"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programming = program\n",
            "Programs = program\n",
            "Acheive = acheiv\n",
            "Acheiving = acheiv\n",
            "enjoyment = enjoy\n",
            "eagerly = eager\n",
            "enjoying = enjoy\n",
            "enjoyed = enjoy\n",
            "History = histori\n",
            "Historical = histor\n",
            "eating = eat\n",
            "eaten = eaten\n",
            "orderly = order\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "yR5q1zzXOSkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmas = WordNetLemmatizer().lemmatize\n",
        "\n",
        "for word in word_list:\n",
        "    print(word, '=', lemmas(word, pos='n'))\n",
        "\n",
        "# pos - Part Of Speech\n",
        "# pos ---> n-noun, v-verb, a-adjective, r-adverb, s-satellite adjective"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDFgPNtzOW_n",
        "outputId": "03dee17b-77a1-4134-aa72-2ee859fdfdca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programming = Programming\n",
            "Programs = Programs\n",
            "Acheive = Acheive\n",
            "Acheiving = Acheiving\n",
            "enjoyment = enjoyment\n",
            "eagerly = eagerly\n",
            "enjoying = enjoying\n",
            "enjoyed = enjoyed\n",
            "History = History\n",
            "Historical = Historical\n",
            "eating = eating\n",
            "eaten = eaten\n",
            "orderly = orderly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmas('goes', 'v'))   # v - verb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG5BBKWhPcVD",
        "outputId": "9d0ca272-973a-4cc3-8cd2-bea4e680580d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go\n"
          ]
        }
      ]
    }
  ]
}